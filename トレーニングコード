# === 学習（LSTMで q10,q50,q90 を同時予測） ===
import os, json, numpy as np, pandas as pd, torch, torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler

FEAT_CSV = "/content/ntt_data/engineered_timeseries.csv"
OUT_DIR  = "/content/ntt_out"; os.makedirs(OUT_DIR, exist_ok=True)

LOOKBACK = 60
QUANTILES = torch.tensor([0.1, 0.5, 0.9], dtype=torch.float32)
USE_WEIGHTS = True  # 外れ日の重みを弱める

# 1) 読み込み
df = pd.read_csv(FEAT_CSV, parse_dates=["date"], index_col="date")

# 2) 特徴列と目的列を定義
reserved = {"y_next_close","y_next_logret","split","big_move","sample_weight"}
all_cols = df.columns.tolist()
feat_cols = [c for c in all_cols if c not in reserved]
target_col = "y_next_close"   # 終値を直接予測（中央値はq50）

# 3) スプリットごとに配列化
def to_np(d): return d[feat_cols].values.astype("float32"), d[target_col].values.astype("float32"), d["sample_weight"].values.astype("float32")

train_df = df[df["split"]=="train"].copy()
val_df   = df[df["split"]=="val"].copy()
test_df  = df[df["split"]=="test"].copy()  # 評価は次セル

Xtr_raw, ytr_raw, wtr_raw = to_np(train_df)
Xva_raw, yva_raw, wva_raw = to_np(val_df)
Xte_raw, yte_raw, wte_raw = to_np(test_df)

# 4) スケーリング（trainでfit→他はtransformのみ）
xsc, ysc = StandardScaler(), StandardScaler()
Xtr = xsc.fit_transform(Xtr_raw)
Xva = xsc.transform(Xva_raw)
Xte = xsc.transform(Xte_raw)

ytr = ysc.fit_transform(ytr_raw.reshape(-1,1)).ravel()
yva = ysc.transform(yva_raw.reshape(-1,1)).ravel()
yte = ysc.transform(yte_raw.reshape(-1,1)).ravel()  # 評価用（次セルでも再計算）

# 5) シーケンス化（val/testは直前スプリットの末尾を文脈として連結）
def make_seq(X, y, L):
    xs, ys = [], []
    for i in range(L, len(X)):
        xs.append(X[i-L:i]); ys.append(y[i])
    return np.array(xs, np.float32), np.array(ys, np.float32)

# train
Xtr_seq, ytr_seq = make_seq(Xtr, ytr, LOOKBACK)
wtr_seq = wtr_raw[LOOKBACK:] if USE_WEIGHTS else np.ones_like(ytr_seq, dtype="float32")

# val（文脈としてtrain末尾Lを前置→最後のlen(val)だけを使用）
Xva_stack = np.vstack([Xtr[-LOOKBACK:], Xva])
yva_stack = np.hstack([ytr[-LOOKBACK:], yva])
Xva_seq_all, yva_seq_all = make_seq(Xva_stack, yva_stack, LOOKBACK)
Xva_seq, yva_seq = Xva_seq_all[-len(yva):], yva_seq_all[-len(yva):]
wva_seq = np.ones_like(yva_seq, dtype="float32")

# test（文脈としてval末尾Lを前置）
Xte_stack = np.vstack([Xva[-LOOKBACK:], Xte])
yte_stack = np.hstack([yva[-LOOKBACK:], yte])
Xte_seq_all, yte_seq_all = make_seq(Xte_stack, yte_stack, LOOKBACK)
Xte_seq, yte_seq = Xte_seq_all[-len(yte):], yte_seq_all[-len(yte):]
wte_seq = np.ones_like(yte_seq, dtype="float32")

# 6) DataLoader
class SeqDS(Dataset):
    def __init__(self, X, y, w):
        self.X = torch.tensor(X); self.y = torch.tensor(y); self.w = torch.tensor(w)
    def __len__(self): return len(self.y)
    def __getitem__(self, i): return self.X[i], self.y[i], self.w[i]

bs = 128
train_dl = DataLoader(SeqDS(Xtr_seq, ytr_seq, wtr_seq), batch_size=bs, shuffle=True)
val_dl   = DataLoader(SeqDS(Xva_seq, yva_seq, wva_seq), batch_size=256, shuffle=False)

# 7) モデル（q10,q50,q90）
class LSTMQuantile(nn.Module):
    def __init__(self, d_in, d_h=64, n_layers=2, dropout=0.2, n_out=3):
        super().__init__()
        self.lstm = nn.LSTM(d_in, d_h, num_layers=n_layers, dropout=dropout, batch_first=True)
        self.head = nn.Sequential(nn.Linear(d_h, 64), nn.ReLU(), nn.Linear(64, n_out))
    def forward(self, x):
        o,_ = self.lstm(x)
        return self.head(o[:,-1,:])  # [B,3]

def pinball_loss(y, yhat, qs, weights=None):
    # y: [B], yhat: [B,3]
    y = y.unsqueeze(1)
    e = y - yhat
    losses = torch.maximum(qs*e, (qs-1)*e)  # [B,3]
    loss_per_sample = losses.mean(dim=1)    # [B]
    if weights is not None:
        loss = (loss_per_sample * weights).mean()
    else:
        loss = loss_per_sample.mean()
    # 量子の交差ペナルティ（q10<=q50<=q90を促す）
    pen = torch.relu(yhat[:,0]-yhat[:,1]).mean() + torch.relu(yhat[:,1]-yhat[:,2]).mean()
    return loss + 0.01*pen

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = LSTMQuantile(d_in=Xtr_seq.shape[-1]).to(device)
opt = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)

best_val = 1e9; patience=7; bad=0
EPOCHS=40
for ep in range(1, EPOCHS+1):
    # train
    model.train(); tr=0.0; ntr=0
    for xb, yb, wb in train_dl:
        xb, yb, wb = xb.to(device), yb.to(device), wb.to(device)
        opt.zero_grad()
        pred = model(xb)
        loss = pinball_loss(yb, pred, QUANTILES.to(device), wb if USE_WEIGHTS else None)
        loss.backward(); opt.step()
        tr += loss.item()*xb.size(0); ntr += xb.size(0)
    tr /= ntr
    # val
    model.eval(); va=0.0; nva=0
    with torch.no_grad():
        for xb, yb, wb in val_dl:
            xb, yb = xb.to(device), yb.to(device)
            pred = model(xb)
            loss = pinball_loss(yb, pred, QUANTILES.to(device), None)
            va += loss.item()*xb.size(0); nva += xb.size(0)
    va /= nva
    print(f"ep{ep:02d} train_pinball={tr:.5f} val_pinball={va:.5f}")
    if va < best_val:
        best_val = va; bad=0
        torch.save(model.state_dict(), f"{OUT_DIR}/lstm_quantile.pt")
    else:
        bad += 1
        if bad >= patience: break

# 8) メタ情報とスケーラ保存
import pickle, json
with open(f"{OUT_DIR}/x_scaler.pkl","wb") as f: pickle.dump(xsc, f)
with open(f"{OUT_DIR}/y_scaler.pkl","wb") as f: pickle.dump(ysc, f)
meta = {
  "feat_csv": FEAT_CSV,
  "feat_cols": feat_cols,
  "target_col": target_col,
  "lookback": LOOKBACK,
  "quantiles": [0.1,0.5,0.9],
  "use_weights": USE_WEIGHTS
}
with open(f"{OUT_DIR}/meta.json","w") as f: json.dump(meta, f, indent=2)
print("保存:", OUT_DIR)
